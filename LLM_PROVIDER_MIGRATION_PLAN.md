# LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼åˆ‡ã‚Šæ›¿ãˆæ©Ÿèƒ½å®Ÿè£…è¨ˆç”»

**ä½œæˆæ—¥**: 2025-10-30
**ç›®çš„**: è¤‡æ•°ã®LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ï¼ˆOpenAIã€Groqç­‰ï¼‰ã‚’æŸ”è»Ÿã«åˆ‡ã‚Šæ›¿ãˆã‚‰ã‚Œã‚‹æ§‹é€ ã«ç§»è¡Œ

---

## ğŸ¯ èƒŒæ™¯ãƒ»ç›®çš„

### ãªãœã“ã®æ©Ÿèƒ½ãŒå¿…è¦ã‹

1. **LLMã®é€²åŒ–ãŒé€Ÿã„**
   - æ–°ã—ã„ãƒ¢ãƒ‡ãƒ«ãŒé »ç¹ã«ãƒªãƒªãƒ¼ã‚¹ã•ã‚Œã‚‹
   - ã‚ˆã‚Šé«˜æ€§èƒ½ãƒ»ä½ã‚³ã‚¹ãƒˆãªãƒ¢ãƒ‡ãƒ«ã«ç´ æ—©ãåˆ‡ã‚Šæ›¿ãˆãŸã„

2. **ã‚³ã‚¹ãƒˆæœ€é©åŒ–**
   - ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã”ã¨ã«ä¾¡æ ¼ãŒç•°ãªã‚‹
   - ç”¨é€”ã«ã‚ˆã£ã¦æœ€é©ãªãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã—ãŸã„

3. **A/Bãƒ†ã‚¹ãƒˆãƒ»æ¯”è¼ƒæ¤œè¨¼**
   - è¤‡æ•°ã®ãƒ¢ãƒ‡ãƒ«ã§å“è³ªãƒ»é€Ÿåº¦ãƒ»ã‚³ã‚¹ãƒˆã‚’æ¯”è¼ƒã—ãŸã„
   - æ®µéšçš„ãªç§»è¡ŒãŒå¿…è¦

---

## ğŸ“Š ç¾çŠ¶

### ç¾åœ¨ã®å®Ÿè£…ï¼ˆ2025-10-30æ™‚ç‚¹ï¼‰

- **å˜ä¸€ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼**: OpenAIå›ºå®š
- **ç¾åœ¨ä½¿ç”¨ä¸­ã®ãƒ¢ãƒ‡ãƒ«**: `gpt-5-nano`
- **ç’°å¢ƒå¤‰æ•°**: `OPENAI_MODEL`ã§æŒ‡å®š
- **å•é¡Œç‚¹**: ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼å¤‰æ›´ã«ã¯å¤§è¦æ¨¡ãªã‚³ãƒ¼ãƒ‰ä¿®æ­£ãŒå¿…è¦

```python
# main.pyï¼ˆç¾çŠ¶ï¼‰
from openai import OpenAI

client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
OPENAI_MODEL = os.getenv("OPENAI_MODEL")  # gpt-5-nano

# å„ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã§ç›´æ¥å‘¼ã³å‡ºã—
response = client.chat.completions.create(
    model=OPENAI_MODEL,
    messages=[{"role": "user", "content": prompt}]
)
```

---

## ğŸ¯ ç›®æ¨™ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

### è¨­è¨ˆæ–¹é‡

**æ¨å¥¨ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ**: å˜ä¸€APIå†…ã§ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼åˆ‡ã‚Šæ›¿ãˆï¼ˆFactory Patternï¼‰

**ãƒ¡ãƒªãƒƒãƒˆ**:
- ã‚³ãƒ¼ãƒ‰ã®é‡è¤‡ã‚’é¿ã‘ã‚‰ã‚Œã‚‹
- 1ã¤ã®ãƒ‡ãƒ—ãƒ­ã‚¤ã§å…¨ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’ç®¡ç†
- ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ™‚ã«ãƒ¢ãƒ‡ãƒ«ã‚’æŒ‡å®šã§ãã‚‹ã®ã§æŸ”è»Ÿ
- A/Bãƒ†ã‚¹ãƒˆã‚„æ¯”è¼ƒãŒå®¹æ˜“

---

## ğŸ—ï¸ å®Ÿè£…è¨­è¨ˆ

### 1. ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼æŠ½è±¡åŒ–ãƒ¬ã‚¤ãƒ¤ãƒ¼

```
api/vibe-analysis/scorer/
â”œâ”€â”€ main.py
â”œâ”€â”€ llm_providers.py          # æ–°è¦ä½œæˆ
â”‚   â”œâ”€â”€ LLMProvider (æŠ½è±¡ã‚¯ãƒ©ã‚¹)
â”‚   â”œâ”€â”€ OpenAIProvider
â”‚   â”œâ”€â”€ GroqProvider
â”‚   â””â”€â”€ LLMFactory
â”œâ”€â”€ supabase_client.py
â””â”€â”€ requirements.txt           # groqä¾å­˜é–¢ä¿‚è¿½åŠ 
```

### 2. ã‚¯ãƒ©ã‚¹æ§‹é€ ï¼ˆllm_providers.pyï¼‰

```python
from abc import ABC, abstractmethod

class LLMProvider(ABC):
    """LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®æŠ½è±¡ã‚¯ãƒ©ã‚¹"""

    @abstractmethod
    async def generate(self, prompt: str) -> str:
        pass

    @property
    @abstractmethod
    def model_name(self) -> str:
        pass

class OpenAIProvider(LLMProvider):
    def __init__(self, model: str = "gpt-4"):
        self.client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        self._model = model

    async def generate(self, prompt: str) -> str:
        response = self.client.chat.completions.create(
            model=self._model,
            messages=[{"role": "user", "content": prompt}]
        )
        return response.choices[0].message.content

    @property
    def model_name(self) -> str:
        return f"openai/{self._model}"

class GroqProvider(LLMProvider):
    def __init__(self, model: str = "llama-3.1-70b-versatile"):
        self.client = Groq(api_key=os.getenv("GROQ_API_KEY"))
        self._model = model

    async def generate(self, prompt: str) -> str:
        response = self.client.chat.completions.create(
            model=self._model,
            messages=[{"role": "user", "content": prompt}]
        )
        return response.choices[0].message.content

    @property
    def model_name(self) -> str:
        return f"groq/{self._model}"

class LLMFactory:
    """LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®ãƒ•ã‚¡ã‚¯ãƒˆãƒªãƒ¼ã‚¯ãƒ©ã‚¹"""

    @staticmethod
    def create(provider: str, model: str = None) -> LLMProvider:
        if provider == "openai":
            return OpenAIProvider(model or "gpt-4")
        elif provider == "groq":
            return GroqProvider(model or "llama-3.1-70b-versatile")
        else:
            raise ValueError(f"Unknown provider: {provider}")

    @staticmethod
    def get_default() -> LLMProvider:
        """ç’°å¢ƒå¤‰æ•°ã‹ã‚‰èª­ã¿è¾¼ã¿"""
        provider = os.getenv("DEFAULT_LLM_PROVIDER", "openai")
        model = os.getenv("DEFAULT_LLM_MODEL", "gpt-4")
        return LLMFactory.create(provider, model)
```

### 3. ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¹ã‚­ãƒ¼ãƒæ‹¡å¼µ

```python
# main.py
class PromptRequest(BaseModel):
    prompt: str
    model_provider: Optional[str] = None  # "openai", "groq"
    model_name: Optional[str] = None      # "gpt-4", "llama-3.1-70b-versatile"

class TimeBlockAnalysisRequest(BaseModel):
    prompt: str
    device_id: Optional[str] = None
    date: Optional[str] = None
    time_block: Optional[str] = None
    model_provider: Optional[str] = None  # è¿½åŠ 
    model_name: Optional[str] = None      # è¿½åŠ 
```

### 4. ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆä¿®æ­£ä¾‹

```python
@app.post("/analyze/chatgpt")
async def relay_to_llm(request: PromptRequest):
    """ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’LLMã«ä¸­ç¶™ï¼ˆãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼é¸æŠå¯èƒ½ï¼‰"""
    try:
        # ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®é¸æŠ
        if request.model_provider and request.model_name:
            llm = LLMFactory.create(request.model_provider, request.model_name)
        elif request.model_provider:
            llm = LLMFactory.create(request.model_provider)
        else:
            llm = LLMFactory.get_default()

        print(f"ğŸ¤– ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {llm.model_name}")

        # LLMå‘¼ã³å‡ºã—
        raw_response = await llm.generate(request.prompt)

        # JSONæŠ½å‡ºå‡¦ç†ï¼ˆæ—¢å­˜ã®ãƒ­ã‚¸ãƒƒã‚¯ï¼‰
        extracted_data = extract_json_from_response(raw_response)
        processed_data = process_nan_values(extracted_data)

        return {
            "result": processed_data,
            "model_used": llm.model_name,
            "timestamp": datetime.now().isoformat()
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
```

---

## ğŸ”§ å®Ÿè£…æ‰‹é †

### Phase 1: ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°ï¼ˆç¾çŠ¶ç¶­æŒï¼‰

1. **llm_providers.pyä½œæˆ**
   - æŠ½è±¡ã‚¯ãƒ©ã‚¹å®šç¾©
   - OpenAIProviderå®Ÿè£…
   - LLMFactoryå®Ÿè£…

2. **main.pyä¿®æ­£**
   - æ—¢å­˜ã®OpenAIå‘¼ã³å‡ºã—ã‚’Factoryãƒ‘ã‚¿ãƒ¼ãƒ³ã«ç½®ãæ›ãˆ
   - å‹•ä½œç¢ºèªï¼ˆæ—¢å­˜æ©Ÿèƒ½ãŒæ­£å¸¸å‹•ä½œã™ã‚‹ã“ã¨ï¼‰

3. **ãƒ†ã‚¹ãƒˆ**
   - æ—¢å­˜ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆå…¨ã¦å‹•ä½œç¢ºèª
   - ãƒ¬ã‚¹ãƒãƒ³ã‚¹å½¢å¼ãŒå¤‰ã‚ã£ã¦ã„ãªã„ã“ã¨ç¢ºèª

### Phase 2: æ–°è¦ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼è¿½åŠ 

4. **Groqè¿½åŠ **
   - GroqProviderå®Ÿè£…
   - requirements.txtæ›´æ–°ï¼ˆ`groq` ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸è¿½åŠ ï¼‰
   - ç’°å¢ƒå¤‰æ•°è¿½åŠ ï¼ˆ`.env`ã«`GROQ_API_KEY`ï¼‰

5. **æ¯”è¼ƒãƒ†ã‚¹ãƒˆ**
   - åŒã˜ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§ä¸¡æ–¹ã®ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’ãƒ†ã‚¹ãƒˆ
   - å“è³ªãƒ»é€Ÿåº¦ãƒ»ã‚³ã‚¹ãƒˆã‚’æ¯”è¼ƒ

6. **ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ›´æ–°**
   - README.mdã«ä½¿ç”¨æ–¹æ³•ã‚’è¿½è¨˜

---

## ğŸ“ ç’°å¢ƒå¤‰æ•°è¨­å®š

### .envãƒ•ã‚¡ã‚¤ãƒ«

```bash
# OpenAIï¼ˆæ—¢å­˜ï¼‰
OPENAI_API_KEY=sk-...
OPENAI_MODEL=gpt-5-nano

# Groqï¼ˆæ–°è¦ï¼‰
GROQ_API_KEY=gsk_...
GROQ_MODEL=llama-3.1-70b-versatile

# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šï¼ˆæ–°è¦ï¼‰
DEFAULT_LLM_PROVIDER=openai  # ã¾ãŸã¯ groq
DEFAULT_LLM_MODEL=gpt-5-nano
```

---

## ğŸ”„ é‹ç”¨æ–¹æ³•

### åˆ‡ã‚Šæ›¿ãˆæ–¹æ³•1: ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ™‚ã«æŒ‡å®šï¼ˆæ¨å¥¨ï¼‰

```bash
# OpenAIä½¿ç”¨
curl -X POST https://api.hey-watch.me/vibe-analysis/scoring/analyze/chatgpt \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "ã“ã‚“ã«ã¡ã¯",
    "model_provider": "openai",
    "model_name": "gpt-4"
  }'

# Groqä½¿ç”¨
curl -X POST https://api.hey-watch.me/vibe-analysis/scoring/analyze/chatgpt \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "ã“ã‚“ã«ã¡ã¯",
    "model_provider": "groq",
    "model_name": "llama-3.1-70b-versatile"
  }'

# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆä½¿ç”¨ï¼ˆç’°å¢ƒå¤‰æ•°ã‹ã‚‰ï¼‰
curl -X POST https://api.hey-watch.me/vibe-analysis/scoring/analyze/chatgpt \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "ã“ã‚“ã«ã¡ã¯"
  }'
```

### åˆ‡ã‚Šæ›¿ãˆæ–¹æ³•2: ç’°å¢ƒå¤‰æ•°ã§ä¸€æ‹¬åˆ‡ã‚Šæ›¿ãˆ

```bash
# EC2ã§.envã‚’ç·¨é›†
ssh -i ~/watchme-key.pem ubuntu@3.24.16.82
vi /home/ubuntu/vibe-analysis-scorer/.env

# DEFAULT_LLM_PROVIDERã‚’groqã«å¤‰æ›´
DEFAULT_LLM_PROVIDER=groq
DEFAULT_LLM_MODEL=llama-3.1-70b-versatile

# ã‚µãƒ¼ãƒ“ã‚¹å†èµ·å‹•
sudo systemctl restart vibe-analysis-scorer
```

---

## âš ï¸ æ³¨æ„äº‹é …

### 1. å¾Œæ–¹äº’æ›æ€§ã®ä¿æŒ

- æ—¢å­˜ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆï¼ˆ`model_provider`ãªã—ï¼‰ã¯å¼•ãç¶šãå‹•ä½œã™ã‚‹ã“ã¨
- ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ç¾åœ¨ã®OpenAIï¼ˆgpt-5-nanoï¼‰ã‚’ç¶­æŒ

### 2. ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°

- ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼APIã®ã‚¨ãƒ©ãƒ¼ã‚’é©åˆ‡ã«ã‚­ãƒ£ãƒƒãƒ
- ãƒªãƒˆãƒ©ã‚¤ãƒ­ã‚¸ãƒƒã‚¯ï¼ˆtenacityï¼‰ã‚’å„ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã«é©ç”¨

### 3. ãƒ¬ã‚¹ãƒãƒ³ã‚¹å½¢å¼ã®çµ±ä¸€

- ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãŒå¤‰ã‚ã£ã¦ã‚‚ãƒ¬ã‚¹ãƒãƒ³ã‚¹å½¢å¼ã¯åŒã˜ã«ã™ã‚‹
- `model_used`ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã§ã©ã®ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ãŸã‹æ˜ç¤º

### 4. ã‚³ã‚¹ãƒˆç®¡ç†

- ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã”ã¨ã®APIä½¿ç”¨é‡ã‚’è¨˜éŒ²
- ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã§ç¢ºèªã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹ï¼ˆå°†æ¥ï¼‰

---

## ğŸ“Š æ¯”è¼ƒåŸºæº–

æ–°ã—ã„ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’è¿½åŠ ã—ãŸã‚‰ã€ä»¥ä¸‹ã‚’æ¯”è¼ƒï¼š

| é …ç›® | OpenAI (gpt-5-nano) | Groq (llama-3.1-70b) |
|------|---------------------|----------------------|
| **ã‚³ã‚¹ãƒˆ** | $/1Mãƒˆãƒ¼ã‚¯ãƒ³ | $/1Mãƒˆãƒ¼ã‚¯ãƒ³ |
| **é€Ÿåº¦** | ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚¿ã‚¤ãƒ  | ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚¿ã‚¤ãƒ  |
| **å“è³ª** | åˆ†æç²¾åº¦ | åˆ†æç²¾åº¦ |
| **å®‰å®šæ€§** | ã‚¨ãƒ©ãƒ¼ç‡ | ã‚¨ãƒ©ãƒ¼ç‡ |

---

## ğŸ“ å½±éŸ¿ç¯„å›²

### ä¿®æ­£ãŒå¿…è¦ãªãƒ•ã‚¡ã‚¤ãƒ«

1. âœ… `/api/vibe-analysis/scorer/llm_providers.py` - æ–°è¦ä½œæˆ
2. âœ… `/api/vibe-analysis/scorer/main.py` - ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°
3. âœ… `/api/vibe-analysis/scorer/requirements.txt` - groqè¿½åŠ 
4. âœ… `/api/vibe-analysis/scorer/.env` - ç’°å¢ƒå¤‰æ•°è¿½åŠ 
5. âœ… `/api/vibe-analysis/scorer/README.md` - ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ›´æ–°

### ä¿®æ­£ä¸è¦ï¼ˆå½±éŸ¿ãªã—ï¼‰

- âŒ Nginxè¨­å®šï¼ˆã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆå¤‰æ›´ãªã—ï¼‰
- âŒ Lambdaé–¢æ•°ï¼ˆAPIã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹å¤‰æ›´ãªã—ï¼‰
- âŒ systemdè¨­å®šï¼ˆç’°å¢ƒå¤‰æ•°ã¯.envã§ç®¡ç†ï¼‰

---

## ğŸš€ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

1. **Phase 1å®Ÿè£…** - OpenAIã‚’Factoryãƒ‘ã‚¿ãƒ¼ãƒ³ã«ç§»è¡Œ
2. **å‹•ä½œç¢ºèª** - æ—¢å­˜æ©Ÿèƒ½ãŒæ­£å¸¸ã«å‹•ä½œã™ã‚‹ã“ã¨ã‚’ç¢ºèª
3. **Phase 2å®Ÿè£…** - Groqè¿½åŠ 
4. **æ¯”è¼ƒæ¤œè¨¼** - ä¸¡æ–¹ã®ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã§å“è³ªãƒ»ã‚³ã‚¹ãƒˆãƒ»é€Ÿåº¦ã‚’æ¯”è¼ƒ
5. **æœ¬ç•ªé©ç”¨** - æœ€é©ãªãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’é¸æŠã—ã¦é‹ç”¨

---

**ä½œæˆè€…**: Claude (Session 2025-10-30)
**æ¬¡ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã¸**: ã“ã®è¨ˆç”»ã«åŸºã¥ã„ã¦å®Ÿè£…ã‚’é€²ã‚ã¦ãã ã•ã„
